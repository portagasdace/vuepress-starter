(window.webpackJsonp=window.webpackJsonp||[]).push([[286],{642:function(a,r,t){"use strict";t.r(r);var p=t(42),e=Object(p.a)({},(function(){var a=this,r=a.$createElement,t=a._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"scrapyd-源码目录"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd-源码目录"}},[a._v("#")]),a._v(" Scrapyd 源码目录")]),a._v(" "),t("p",[a._v("前面学习了 Scrapyd 的简介、文档及安装使用，基础部分已经学习完毕，或许你还有许多的疑问:")]),a._v(" "),t("ul",[t("li",[a._v("API 的功能是如何实现的？")]),a._v(" "),t("li",[a._v("Jobs 页面的爬虫运行状态是怎么统计出来的？")]),a._v(" "),t("li",[a._v("爬虫日志是如何生成的？")]),a._v(" "),t("li",[a._v("Scrapyd 配置文件在哪里？")]),a._v(" "),t("li",[a._v("Scrapyd 源码目录结构是什么样的？")]),a._v(" "),t("li",[a._v("……")])]),a._v(" "),t("p",[a._v("带着这些疑问，随我到源码中探个究竟。")]),a._v(" "),t("h2",{attrs:{id:"scrapyd-的源码"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd-的源码"}},[a._v("#")]),a._v(" Scrapyd 的源码")]),a._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/scrapy/scrapyd",target:"_blank",rel:"noopener noreferrer"}},[a._v("Scrapyd 的源码"),t("OutboundLink")],1),a._v("可以在 GitHub 上找到：")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/11/16661148609dd8ff?w=1374&h=954&f=gif&s=4190031",alt:""}})]),a._v(" "),t("p",[a._v("它的文件目录如下图所示：")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/4/1663ef710d2c284a?w=975&h=632&f=png&s=97838",alt:""}})]),a._v(" "),t("p",[a._v("从 GitHub 的代码提交记录来看，最近的代码更新是在 11个月以前，并且几乎每年都会有更新。代表这个项目是持续更新维护的，不用担心使用过后官方团队停止维护导致的项目管理隐患。")]),a._v(" "),t("p",[a._v("实际上我们安装 Scrapyd 之后，用到的代码是在上图"),t("code",[a._v("scrapyd")]),a._v("目录中，安装时也相当于将 Scrapyd 目录复制到 site-packages 内，至于其他的目录和文件，是在 PyPI 打包提交以及编写文档时所生成的文件。")]),a._v(" "),t("h2",{attrs:{id:"启动文件、视图及-api"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启动文件、视图及-api"}},[a._v("#")]),a._v(" 启动文件、视图及 API")]),a._v(" "),t("p",[a._v("进入 Scrapyd 目录，下图为 Scrapyd 目录下的所有文件：")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/4/1663efc8353cee6b?w=981&h=833&f=png&s=132063",alt:""}})]),a._v(" "),t("p",[a._v("这个小节我们"),t("strong",[a._v("主要了解一下各个文件或目录的大致作用，知晓目录的结构以及文件分布")]),a._v("。")]),a._v(" "),t("p",[a._v("至于启动文件、视图、API 文件的源码调试与解析会在后面的小节中重点讲解，以下列出每个文件或目录的作用释义：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("* scripts - 里面只有 1个可用的文件：scrapyd_run.py,\n它是整个项目的启动文件。\n* init.py - Scrapyd 启动前的配置。\n* tests - 用于测试 Scrapyd 功能的代码集。\n* VERSION - Scrapyd 版本号文件。\n* app.py - Scrapyd 应用配置文件读取并进行设置。\n* config.py - Scrapyd 配置及相关设置。\n* default_scrapyd.conf -  Scrapyd 配置文件\n* eggstorage.py - 项目打包设置及版本生成。\n* eggutils.py - 项目打包。\n* environ.py - 项目打包目录读取\n* interfaces.py - 处理和存储项目包的检索以及爬虫队列。\n* launcher.py - 执行爬虫运行、记录状态以及进程池相关。\n* poller.py - 队列相关。\n* runner.py - 任务具体执行。\n* scheduler.py - 任务及项目的状态与记录更新。\n* script.py - 用于执行 Scrapy 传递的命令。\n* spiderqueue.py - 爬虫队列相关。\n* sqllite.py - sqllite 数据库相关。\n* txapp.py - 通过 Twisted 启动 Scrapyd。\n* utils.py - 爬虫队列、项目列表 i 以及 JSON 视图。\n* webservice.py - JSON 视图以及 Scrapyd 中提供的所有 API。\n* website.py - Web 视图、Home、Jobs 等页面及功能。\n\n")])])]),t("h2",{attrs:{id:"启动文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启动文件"}},[a._v("#")]),a._v(" 启动文件")]),a._v(" "),t("p",[a._v("启动文件是整个项目的入口。用于启动项目的文件位于"),t("code",[a._v("/scrapyd/script")]),a._v("目录下，文件名为"),t("code",[a._v("scrapyd_run.py")]),a._v("，它通过载入指定"),t("code",[a._v("txapp.py")]),a._v("文件来启动"),t("code",[a._v("scrapyd")]),a._v("项目，"),t("code",[a._v("scrapyd_run.py")]),a._v("代码如下：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from twisted.scripts.twistd import run\nfrom os.path import join, dirname\nfrom sys import argv\nimport scrapyd\n\ndef main():\n    argv[1:1] = ['-n', '-y', join(dirname(scrapyd.__file__), 'txapp.py')]\nrun()\n\n")])])]),t("p",[t("a",{attrs:{href:"http://txapp.py",target:"_blank",rel:"noopener noreferrer"}},[a._v("txapp.py"),t("OutboundLink")],1),a._v(" 则通过调用 get_application 来获取 Scrapyd 配置并通过 RUN 命令启动 Scrapyd，负责初始化 Scrapyd 的 "),t("a",{attrs:{href:"http://txapp.py",target:"_blank",rel:"noopener noreferrer"}},[a._v("txapp.py"),t("OutboundLink")],1),a._v(" 代码如下：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from scrapyd import get_application\napplication = get_application()\n\n")])])]),t("p",[a._v("关于文件源码的深层次剖析与调试，会在后面的章节《"),t("a",{attrs:{href:"https://juejin.im/book/5bb5d3fa6fb9a05d2a1d819a/section/5bbb471a5188255c5e66fef7",target:"_blank",rel:"noopener noreferrer"}},[a._v("动手调试 Scrapyd 代码"),t("OutboundLink")],1),a._v("》，通过代码调试的方式理解 Scrapyd 的代码运行流程与逻辑。")])])}),[],!1,null,null,null);r.default=e.exports}}]);