(window.webpackJsonp=window.webpackJsonp||[]).push([[277],{633:function(t,s,a){"use strict";a.r(s);var r=a(42),n=Object(r.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"api-代码编写逻辑与流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#api-代码编写逻辑与流程"}},[t._v("#")]),t._v(" API 代码编写逻辑与流程")]),t._v(" "),a("p",[t._v("既然 Web 界面中新增了如此多且重要的功能，那么 API 也不能落后啊。并且 API 作为整个 Scrapyd 或者说 ScrapydArt 的规划重点是有一定的道理的，JSON 格式的数据无论传输或者后续扩展方面都比 HTML 的灵活，并且在使用上也是非常的方便。")]),t._v(" "),a("h2",{attrs:{id:"新增-api-说明"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#新增-api-说明"}},[t._v("#")]),t._v(" 新增 API 说明")]),t._v(" "),a("p",[t._v("根据之前对 ScrapydArt 平台的功能规划：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    + schedulelist.json-用以获取爬虫调用情况。\n    + runtimestats.json-用以获取爬虫运行时间统计。\n    + psnstats.json-用以获取爬虫项目及其数量统计。\n    + prospider.json-用以获取项目与对应爬虫名以及数量统计。\n    + timerank.json-用以获取爬虫运行时长榜。\n    + invokerank.json-用以获取爬虫调用排行榜。\n    + filter.json-根据参数按时间范围/项目名称/爬虫名称/运行时长对爬虫运行记录进行筛选过滤。\n    + order.json-根据参数按时间范围/项目名称/爬虫名称/运行时长对爬虫运行记录进行排序。\n\n")])])]),a("p",[t._v("可以看出，一部分功能代码已经在 Web 重构时编写好了，剩下 filter.json、order.json 的功能需要从头开始编写。")]),t._v(" "),a("h2",{attrs:{id:"实现流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#实现流程"}},[t._v("#")]),t._v(" 实现流程")]),t._v(" "),a("p",[t._v("之前已经尝试编写过一个功能简单的 API，对 API 的编写已经有了具体的实践与经验，但这里还是回顾一下编写一个 API 的流程：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676d99db03a1ca?w=1644&h=147&f=png&s=22990",alt:""}})]),t._v(" "),a("p",[t._v("通过上面的流程图可以发现，整个 API 的编写过程很简单，只需要将具体的功能性代码编写好，然后使用 JSON 格式将数据结果返回，最后到配置文件中添加 API 的路由映射配置即可。")])])}),[],!1,null,null,null);s.default=n.exports}}]);