(window.webpackJsonp=window.webpackJsonp||[]).push([[281],{637:function(t,r,a){"use strict";a.r(r);var e=a(42),p=Object(e.a)({},(function(){var t=this,r=t.$createElement,a=t._self._c||r;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"项目总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#项目总结"}},[t._v("#")]),t._v(" 项目总结")]),t._v(" "),a("p",[t._v("经过一段时间的学习，相信你已经可以熟练的对 Scrapyd 进行改造了。让我们来回顾一下这个由浅入深的学习过程：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("* 首先，我们在最开始的第 1-2 节了解到爬虫部署神器 Scrapyd，并且学会了安装和启动；\n* 接下来在第 3 节中使用 Scrapyd-client 将爬虫打包并部署到指定的服务器上，并且尝试使用多个 deploy 配置；\n* 然后在第 4/5/6/7 节熟悉 Scrapyd 的源码目录以及配置文件，对 Scrapyd 有了一个整体的概观；\n* 接下来在第 8/9/10 节中通过阅读源码和代码调试来了解 Scrapyd 的文件结构以及类的作用；\n* 在第 11 节中动手实践，编写了一个视图类，这是我们迈出去的第一个大踏步；\n* 跟着迈出第二步，为 Scrapyd 新增功能性 API；\n* 有了前面对 Scrapyd 的了解，并且结合实际的爬虫工程师需求对新的平台功能进行了规划\n* Scrapyd 最被人吐槽的就是没有访问权限控制，那我们就用装饰器的方式为其添加访问权控制；\n* 开始对首页进行大刀阔斧的改造，新增加了许多实用的功能\n* 既然 Web 有了新功能，那么 API 也应该有啊，于是将 Web 的功能 copy 给 API；\n* API 作为重点，应当具备更高级、更复杂的功能，于是就开发了 Filter 与 Order 这两个 API；\n\n")])])]),a("p",[t._v("在此过程中，我们收获了阅读项目源码的能力，并且能够通过调试代码加深对项目的理解，增强编码能力。")]),t._v(" "),a("h2",{attrs:{id:"资源汇总"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#资源汇总"}},[t._v("#")]),t._v(" 资源汇总")]),t._v(" "),a("p",[t._v("ScrapydArt 源码："),a("a",{attrs:{href:"https://github.com/dequinns/ScrapydArt",target:"_blank",rel:"noopener noreferrer"}},[t._v("GitHub ScrapydArt"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cfe3fcbcd5db?w=984&h=477&f=png&s=78050",alt:""}})]),t._v(" "),a("p",[t._v("ScrapydArt 文档："),a("a",{attrs:{href:"https://scrapydart.readthedocs.io/zh/latest/",target:"_blank",rel:"noopener noreferrer"}},[t._v("文档"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cfdba375d1fb?w=1205&h=726&f=png&s=140177",alt:""}})]),t._v(" "),a("p",[t._v("ScrapydArt 安装包："),a("a",{attrs:{href:"https://pypi.org/project/scrapydart/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pypi"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cff5d31899ce?w=1318&h=294&f=png&s=25147",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"其他管理平台"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其他管理平台"}},[t._v("#")]),t._v(" 其他管理平台")]),t._v(" "),a("p",[t._v("即使是 ScrapydArt，也仅仅能够满足日常的爬虫管理需求，但是要做到调度、监控和可配置，那还是远远不够的。那么有没有功能更为丰富的平台呢？")]),t._v(" "),a("blockquote",[a("p",[t._v("Gerapy")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cf3efeab174f?w=200&h=200&f=png&s=8303",alt:""}})]),t._v(" "),a("p",[t._v("Gerapy 是一款分布式爬虫管理框架，支持 Python 3，基于 Scrapy、Scrapyd、Scrapyd-Client、Scrapy-Redis、Scrapyd-API、Scrapy-Splash、Jinjia2、Django、Vue.js 开发，Gerapy 可以帮助我们：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("* 更方便地控制爬虫运行\n* 更直观地查看爬虫状态\n* 更实时地查看爬取结果\n* 更简单地实现项目部署\n* 更统一地实现主机管理\n* 更轻松地编写爬虫代码\n\n")])])]),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cf2b51779c01?w=2048&h=1030&f=jpeg&s=259220",alt:""}})]),t._v(" "),a("p",[t._v("从 Scrapy 的部署、启动到监控、日志查看，我们只需要鼠标键盘点几下就可以完成那岂不是美滋滋？更或者说，连 Scrapy 代码都可以帮你自动生成，那岂不是爽爆了？")]),t._v(" "),a("p",[t._v("有需求就有动力，没错，Gerapy 就是为此而生的，GitHub："),a("a",{attrs:{href:"https://github.com/Gerapy/Gerapy",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/Gerapy/Gerapy"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("这是由《Python 3 网络爬虫开发实战》一书的作者崔庆才主导的项目，已经有非常多的公司和爬虫技术团队在使用，笔者也加入了 Gerapy 的开发小组为 Gerapy 贡献力量。")]),t._v(" "),a("blockquote",[a("p",[t._v("SpiderKeeper")])]),t._v(" "),a("p",[t._v("与 Gerapy 相似，它同样提供了爬虫的管理与调度服务")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/16/1667cf56521534c5?w=2558&h=1450&f=png&s=178199",alt:""}})]),t._v(" "),a("p",[a("strong",[t._v("现在你也拥有了打造爬虫部署管理控制台的能力，你将会为自己定制哪些功能呢？")])])])}),[],!1,null,null,null);r.default=p.exports}}]);