(window.webpackJsonp=window.webpackJsonp||[]).push([[287],{643:function(e,s,r){"use strict";r.r(s);var a=r(42),c=Object(a.a)({},(function(){var e=this,s=e.$createElement,r=e._self._c||s;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h1",{attrs:{id:"scrapyd-配置文件详解"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd-配置文件详解"}},[e._v("#")]),e._v(" Scrapyd 配置文件详解")]),e._v(" "),r("p",[e._v("Scrapyd 的配置文件是非常重要的，配置文件中除了日志目录、日志数量、并发数等基础设置外还包括端口设置、IP 绑定设置、API 路由配置等应用层面的设置。")]),e._v(" "),r("p",[e._v("我们需要对 Scrapyd 的配置文件有清晰的认知，避免在认知模糊的情况下对其配置文件进行改动。而且在后续的小节中，也会介绍到如何在代码中读取配置文件以及对应的配置参数值。")]),e._v(" "),r("h2",{attrs:{id:"配置文件参数详解"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#配置文件参数详解"}},[e._v("#")]),e._v(" 配置文件参数详解")]),e._v(" "),r("p",[e._v("从之前的小节中我们可以看到，Scrapyd 会在几个核心类中读取 Scrapyd 的配置文件，获取相应的配置参数。那么，配置文件存放在哪里呢？")]),e._v(" "),r("p",[e._v("待你在 Python 环境下安装好 Scrapyd 后，它会在 Python 安装路径下的"),r("code",[e._v("site-packges")]),e._v("目录下的"),r("code",[e._v("scrapyd")]),e._v("目录内，配置文件名为"),r("code",[e._v("default_scrapyd.conf")]),e._v("，其内容如下：")]),e._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",{pre:!0,attrs:{class:"language-text"}},[r("code",[e._v("[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   =\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 4\nfinished_to_keep = 100\npoll_interval = 5.0\nbind_address = 127.0.0.1\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\nwebroot     = scrapyd.website.Root\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\ndaemonstatus.json = scrapyd.webservice.DaemonStatus\n\n")])])]),r("p",[e._v("它分为 "),r("strong",[e._v("Scrapyd")]),e._v(" 和 "),r("strong",[e._v("Services")]),e._v(" 两种级别，其中每个参数的作用，在 "),r("a",{attrs:{href:"https://scrapyd.readthedocs.io/en/stable/config.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Scrapyd 官方文档"),r("OutboundLink")],1),e._v("中有详细的介绍。")]),e._v(" "),r("h3",{attrs:{id:"scrapyd-级配置"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd-级配置"}},[e._v("#")]),e._v(" Scrapyd 级配置")]),e._v(" "),r("ul",[r("li",[e._v("其中"),r("code",[e._v(".._dir")]),e._v("均为对应的目录设置，如"),r("code",[e._v("eggs_dir、logs_dir、items_dir、dbs_dir")]),e._v("。")]),e._v(" "),r("li",[r("code",[e._v("jobs_to_keep")]),e._v(" - 保存每个爬虫的日志记录数，默认为 5，你可以设置为 500。")]),e._v(" "),r("li",[r("code",[e._v("max_proc")]),e._v(" - Scrapyd 最大并发数，默认 0 时使用"),r("code",[e._v("max_proc_per_cpu")]),e._v("设置的数量。")]),e._v(" "),r("li",[r("code",[e._v("max_proc_per_cpu")]),e._v(" - 每个 CPU 最大并发数设置，默认为 4.")]),e._v(" "),r("li",[r("code",[e._v("finished_to_keep")]),e._v(" - 在 launcher 中保持的进程数，默认 100.")]),e._v(" "),r("li",[r("code",[e._v("poll_interval")]),e._v(" - 轮询队列的间隔，默认 5秒，可以设置 0.2 或其它 int/float 数值。")]),e._v(" "),r("li",[r("code",[e._v("bind_address")]),e._v(" - 默认值 127.0.0.1，只允许本机访问，如果想要开启远程访问改为 0.0.0.0 即可。")]),e._v(" "),r("li",[r("code",[e._v("http_port")]),e._v(" - http 端口，默认 6800。")]),e._v(" "),r("li",[r("code",[e._v("debug")]),e._v(" - Debug 调试开关。")]),e._v(" "),r("li",[r("code",[e._v("runner、application、launcher、webroot")]),e._v(" - 关联的是 Scrapyd 项目代码中编写的对应对象。")])]),e._v(" "),r("blockquote",[r("p",[e._v("以上配置只能写在 Scrapyd 级配置中，如果写错了或者写到其他位置，Scrapyd 在读取配置文件的时候是读取不到的。")])]),e._v(" "),r("h3",{attrs:{id:"services-级配置"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#services-级配置"}},[e._v("#")]),e._v(" Services 级配置")]),e._v(" "),r("p",[e._v("Services 级专用于配置 API 的路由映射关系，将 URL 地址与资源关联起来。")]),e._v(" "),r("p",[e._v("假如在 "),r("code",[e._v("webservice.py")]),e._v(" 中新增一个名为 Caps 的类，那么在配置文件中的写法应与其他类的路由写法一致：")]),e._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",{pre:!0,attrs:{class:"language-text"}},[r("code",[e._v("[services]\nschedule.json     = scrapyd.webservice.Schedule\n... ...\ndaemonstatus.json = scrapyd.webservice.DaemonStatus\ncaps.json = scrapyd.webservice.Caps\n\n")])])]),r("blockquote",[r("p",[e._v("下面这张 GIF 动图演示了整个 API 代码编写、路由配置以及 API 使用的过程。")])]),e._v(" "),r("p",[r("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/11/166613776ef306c2?w=1363&h=862&f=gif&s=4961299",alt:""}})]),e._v(" "),r("h2",{attrs:{id:"小结"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[e._v("#")]),e._v(" 小结")]),e._v(" "),r("p",[e._v("本小节我们学习了 Scrapyd 配置文件中各个参数的作用以及配置文件级别的区别，并且通过编写 API 的案例对 Scrapyd 配置文件进行了设置。有了实践经验后，我们就可以在后面的小节中对 Scrapyd 的配置文件进行自定义设置了。")])])}),[],!1,null,null,null);s.default=c.exports}}]);