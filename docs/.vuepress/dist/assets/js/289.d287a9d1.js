(window.webpackJsonp=window.webpackJsonp||[]).push([[289],{645:function(e,s,t){"use strict";t.r(s);var a=t(42),r=Object(a.a)({},(function(){var e=this,s=e.$createElement,t=e._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"官方源码剖析与-api-详解"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#官方源码剖析与-api-详解"}},[e._v("#")]),e._v(" 官方源码剖析与 API 详解")]),e._v(" "),t("p",[e._v("本小节我们将对 Scrapyd 原有的 API 做个详细的剖析，只有在了解它原有设计思想与代码风格后，我们才能够照猫画虎，设计出风格相似的代码模块，为之后我们自定义 API 和编写权限验证功能打下基础。")]),e._v(" "),t("blockquote",[t("p",[e._v("API 相关代码在 "),t("code",[e._v("webservice.py")]),e._v(" 文件中。")])]),e._v(" "),t("p",[e._v("根据前面的小节，我们知道 Scrapyd 的视图分为 HTML 和 JSON 两种。我们所看到的数据呈现都是由视图类处理的，比如:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("* 当我们访问根目录的时候，对应的的是 Home 这个类；\n* 而访问 Jobs 的时候，对应的的是 Jobs 这个类；\n* 而爬虫的 API 则是，如启动爬虫的 Schedule 类和查看爬虫列表的 ListSpiders 类。\n\n")])])]),t("h2",{attrs:{id:"html-视图"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#html-视图"}},[e._v("#")]),e._v(" HTML 视图")]),e._v(" "),t("p",[e._v("HTML 视图类都写在 Scrapyd 目录下的"),t("code",[e._v("website.py")]),e._v("文件中，里面有三个类："),t("code",[e._v("Root")]),e._v("、"),t("code",[e._v("Home")]),e._v("、"),t("code",[e._v("Jobs")]),e._v("。")]),e._v(" "),t("h3",{attrs:{id:"root-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#root-类"}},[e._v("#")]),e._v(" Root 类")]),e._v(" "),t("p",[e._v("Root 完成了"),t("code",[e._v("Web")]),e._v("路由设置，Scrapyd 一些基础配置的读取设置，日志目录和"),t("code",[e._v("Item")]),e._v("目录与路由配置、项目信息更新等任务，是 Scrapyd 的重要组成部分，其代码结构如下图所示：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/17/16680fc95682b5f5?w=726&h=735&f=png&s=28547",alt:""}})]),e._v(" "),t("h4",{attrs:{id:"web-路由设置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#web-路由设置"}},[e._v("#")]),e._v(" Web 路由设置")]),e._v(" "),t("p",[e._v("在 Root 类的"),t("code",[e._v("__init__")]),e._v("方法中 Home 类以及 Jobs 类的路由是通过 Twisted 的 putChild 进行配置的：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("self.putChild(b'jobs', Jobs(self, local_items))\nself.putChild(b'', Home(self, local_items))\n\n")])])]),t("h4",{attrs:{id:"日志与-item-目录配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#日志与-item-目录配置"}},[e._v("#")]),e._v(" 日志与 Item 目录配置")]),e._v(" "),t("p",[e._v("同样在"),t("code",[e._v("__init__")]),e._v("方法中，先读取日志与 Item 的目录：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("logsdir = config.get('logs_dir')\nitemsdir = config.get('items_dir')\n\n")])])]),t("p",[e._v("然后为它们设置路由：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("self.putChild(b'items', static.File(itemsdir, 'text/plain'))\nself.putChild(b'logs', static.File(logsdir.encode('ascii', 'ignore'), 'text/plain'))\n\n")])])]),t("h4",{attrs:{id:"项目信息更新"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#项目信息更新"}},[e._v("#")]),e._v(" 项目信息更新")]),e._v(" "),t("p",[e._v("当项目变动，比如增删 projects，就会通过 "),t("code",[e._v("update_projects()")]),e._v(" 方法进行更新：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def update_projects(self):\n    self.poller.update_projects()\n    self.scheduler.update_projects()\n@property\ndef poller(self):\n    return self.app.getComponent(IPoller)\n    \n@property\ndef scheduler(self):\n    return self.app.getComponent(ISpiderScheduler)\n\n\n")])])]),t("h4",{attrs:{id:"scrapyd-的其他一些基础配置等"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd-的其他一些基础配置等"}},[e._v("#")]),e._v(" Scrapyd 的其他一些基础配置等")]),e._v(" "),t("p",[e._v("比如读取配置文件并且将其赋值给变量：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("self.debug = config.getboolean('debug', False)\nself.runner = config.get('runner')\nservices = config.items('services', ())\n\n")])])]),t("h3",{attrs:{id:"home-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#home-类"}},[e._v("#")]),e._v(" Home 类")]),e._v(" "),t("p",[e._v("Home 负责 Scrapyd 首页的呈现，当我们访问"),t("code",[e._v("http://localhost:6800")]),e._v(" 时看到的界面，就是访问 Home 类，它完成了页面 HTML 布局以及当前已有项目"),t("code",[e._v("projects")]),e._v("的名称列表展示。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/17/16681024695fa0a3?w=750&h=332&f=png&s=15483",alt:""}})]),e._v(" "),t("h4",{attrs:{id:"init-方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-方法"}},[e._v("#")]),e._v(" "),t("code",[e._v("__init__")]),e._v(" 方法")]),e._v(" "),t("p",[e._v("Home 类继承自"),t("code",[e._v("resource.Resource")]),e._v("，并且重写了"),t("code",[e._v("__init__")]),e._v("方法，以定义一个 Web 可访问资源：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def __init__(self, root, local_items):\n    resource.Resource.__init__(self)\n    self.root = root\n    self.local_items = local_items\n\n")])])]),t("h4",{attrs:{id:"对于页面呈现的建议"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#对于页面呈现的建议"}},[e._v("#")]),e._v(" 对于页面呈现的建议")]),e._v(" "),t("p",[e._v("Rource 中 render 对于页面呈现的建议：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('"""\nI delegate to methods of self with the form \'render_METHOD\'\nwhere METHOD is the HTTP that was used to make the\nrequest. Examples: render_GET, render_HEAD, render_POST, and\nso on. Generally you should implement those methods instead of\noverriding this one.\n"""\n\n')])])]),t("h4",{attrs:{id:"html-布局的实现以及已有项目列表展示"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#html-布局的实现以及已有项目列表展示"}},[e._v("#")]),e._v(" HTML 布局的实现以及已有项目列表展示")]),e._v(" "),t("p",[e._v("使用 render 建议的方式，用"),t("code",[e._v("render_GET")]),e._v("来完成页面的呈现：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def render_GET(self, txrequest):\n   vars = {'projects': ', '.join(self.root.scheduler.list_projects())}\n   s = \"…… ……\"\n   return s.encode('utf-8')\n\n")])])]),t("h3",{attrs:{id:"jobs-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#jobs-类"}},[e._v("#")]),e._v(" Jobs 类")]),e._v(" "),t("p",[e._v("Jobs 负责 Scrapyd 首页爬虫运行状态展示、日志记录以及取消爬虫运行，当我们访问"),t("code",[e._v("http://localhost:6800/jobs/")]),e._v("时看到的界面，就是 Jobs 类。同样，它也是继承了 "),t("code",[e._v("resource.Resource")]),e._v("。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/17/16681085d3992657?w=739&h=927&f=png&s=38600",alt:""}})]),e._v(" "),t("h4",{attrs:{id:"cancel-与-header"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cancel-与-header"}},[e._v("#")]),e._v(" Cancel 与 Header")]),e._v(" "),t("p",[e._v("它设定取消按钮以及爬虫运行信息的列名：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("cancel_button = \"\"\"\n<form method=\"post\" action=\"/cancel.json\">\n   ……\n      ……\n\"\"\".format\n\nheader_cols = [\n    'Project', 'Spider', 'Job', 'PID', 'Start', \n    'Runtime', 'Finish', 'Log', 'Items', 'Cancel',\n]\n\n")])])]),t("h4",{attrs:{id:"设定-css-样式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#设定-css-样式"}},[e._v("#")]),e._v(" 设定 CSS 样式")]),e._v(" "),t("p",[e._v("为爬虫日志表格设定 CSS 样式：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def gen_css(self):\n    css = [\n        '#jobs>thead td {text-align: center; font-weight: bold}',\n        '#jobs>tbody>tr:first-child {background-color: #eee}']\n    ……\n    return '\\n'.join(css)\n\n")])])]),t("h4",{attrs:{id:"生成表头"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成表头"}},[e._v("#")]),e._v(" 生成表头")]),e._v(" "),t("p",[e._v("生成爬虫日志表格的表头数据：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def prep_row(self, cells):\n    if not isinstance(cells, dict):\n        assert len(cells) == len(self.header_cols)\n    else:\n        cells = [cells.get(k) for k in self.header_cols]\n    cells = ['<td>%s</td>' % ('' if c is None else c) for c in cells]\n    return '<tr>%s</tr>' % ''.join(cells)\n\n")])])]),t("h4",{attrs:{id:"pending-状态爬虫列表"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pending-状态爬虫列表"}},[e._v("#")]),e._v(" Pending 状态爬虫列表")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def prep_tab_pending(self):\n    return '\\n'.join(\n        ……\n    )\n\n")])])]),t("h4",{attrs:{id:"正在运行的爬虫列表"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#正在运行的爬虫列表"}},[e._v("#")]),e._v(" 正在运行的爬虫列表")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def prep_tab_running(self):\n    return '\\n'.join(\n        ……\n        for p in self.root.launcher.processes.values()\n    )\n\n")])])]),t("h4",{attrs:{id:"运行完毕的爬虫列表及日志记录"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#运行完毕的爬虫列表及日志记录"}},[e._v("#")]),e._v(" 运行完毕的爬虫列表及日志记录")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def prep_tab_finished(self):\n    return '\\n'.join(\n        ……\n        for p in self.root.launcher.finished\n    )\n\n")])])]),t("h4",{attrs:{id:"生成爬虫运行信息表"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成爬虫运行信息表"}},[e._v("#")]),e._v(" 生成爬虫运行信息表")]),e._v(" "),t("p",[e._v("调用以上功能，生成爬虫运行信息表格：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def prep_table(self):\n    return (\n            '<table id=\"jobs\" border=\"1\">'\n            self.prep_row(self.header_cols)\n            self.prep_tab_pending()\n            self.prep_tab_running()\n            self.prep_tab_finished()\n            '…… ……'\n            )\n\n")])])]),t("h4",{attrs:{id:"页面渲染"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#页面渲染"}},[e._v("#")]),e._v(" 页面渲染")]),e._v(" "),t("p",[e._v("使用 render 方法，将生成的爬虫运行信息呈现到"),t("code",[e._v("localhost:6800/jobs/")]),e._v("页面：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("def render(self, txrequest):\n    doc = self.prep_doc()\n    txrequest.setHeader('Content-Type', 'text/html; charset=utf-8')\n    txrequest.setHeader('Content-Length', len(doc))\n    return doc.encode('utf-8')\n\n")])])]),t("h2",{attrs:{id:"json-视图"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#json-视图"}},[e._v("#")]),e._v(" JSON 视图")]),e._v(" "),t("p",[e._v("JSON 视图类都写在 Scrapyd 目录下的"),t("code",[e._v("webservice.py")]),e._v("文件中，里面除了官方文档中提到的 API 外，还有它们父类"),t("code",[e._v("WsResource")]),e._v("。 这里我挑选 3个 API 对应的类进行讲解。")]),e._v(" "),t("h3",{attrs:{id:"listprojects-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#listprojects-类"}},[e._v("#")]),e._v(" ListProjects 类")]),e._v(" "),t("p",[e._v("ListProjects 类的作用是返回当前 Scrapyd 服务器上的爬虫项目列表。")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('class ListProjects(WsResource):\n\n    def render_GET(self, txrequest):\n        projects = list(self.root.scheduler.list_projects())\n        return {"node_name": self.root.nodename, "status": "ok", "projects": projects}\n\n\n')])])]),t("p",[e._v("它使用 "),t("code",[e._v("render_GET")]),e._v(" 方法，所以在浏览器可以输入"),t("code",[e._v("http://localhost:6800/listprojects.json")]),e._v("来访问并获得 JSON 格式的结果，比如：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('{"status": "ok", "projects": ["AlibabaProject", "JDProject"]}\n\n')])])]),t("h3",{attrs:{id:"listspiders-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#listspiders-类"}},[e._v("#")]),e._v(" ListSpiders 类")]),e._v(" "),t("p",[e._v("ListSpiders 的作用是返回当前 Scrapyd 服务器上指定项目名的爬虫列表。")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('class ListSpiders(WsResource):\n\n    def render_GET(self, txrequest):\n        args = native_stringify_dict(copy(txrequest.args), keys_only=False)\n        project = args[\'project\'][0]\n        version = args.get(\'_version\', [\'\'])[0]\n        spiders = get_spider_list(project, runner=self.root.runner, version=version)\n        return {"node_name": self.root.nodename, "status": "ok", "spiders": spiders}\n\n')])])]),t("p",[e._v("它也是使用"),t("code",[e._v("render_GET")]),e._v("方法，但是它需要携带项目名称作为请求参数。如："),t("code",[e._v("http://localhost:6800/listspiders.json?project=AlibabaProject")]),e._v("返回的结果同样是 json 格式，通过返回的 json 数据，我们知道 AlibabaProject 项目中当前有哪些爬虫。")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('{"status": "ok", "spiders": ["taobao", "1688", "tmall"]}\n\n')])])]),t("h3",{attrs:{id:"deleteproject-类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#deleteproject-类"}},[e._v("#")]),e._v(" DeleteProject 类")]),e._v(" "),t("p",[e._v("DeleteProject 的作用是用于删除 Scrapyd 上已有的爬虫项目。")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('class DeleteProject(WsResource):\n    \n        def render_POST(self, txrequest):\n            args = native_stringify_dict(copy(txrequest.args), keys_only=False)\n            project = args[\'project\'][0]\n            self._delete_version(project)\n            UtilsCache.invalid_cache(project)\n            return {"node_name": self.root.nodename, "status": "ok"}\n    \n        def _delete_version(self, project, version=None):\n            self.root.eggstorage.delete(project, version)\n            self.root.update_projects()\n\n')])])]),t("p",[e._v("它使用的则是"),t("code",[e._v("render_POST")]),e._v("方法，所以请求的时候我们必须以 post 方式进行，并且它要求携带项目名称作为参数值，如:"),t("code",[e._v("http://localhost:6800/delproject.json -d project=AlibabaProject")]),e._v("在成功删除项目后，更新爬虫项目列表，")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("self.root.update_projects()\n\n")])])]),t("p",[e._v("如果操作全部成功，我们得到的响应为：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('{"status": "ok"}\n\n')])])]),t("h2",{attrs:{id:"视图父类与-resource"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#视图父类与-resource"}},[e._v("#")]),e._v(" 视图父类与 Resource")]),e._v(" "),t("p",[e._v("JSON 视图和 HTML 视图的父类是不同的。在 HTML 视图部分，Home 和 Jobs 都继承自"),t("code",[e._v("resource.Resource")]),e._v("，而 JSON 视图部分则继承自"),t("code",[e._v("WsResource")]),e._v("。")]),e._v(" "),t("h3",{attrs:{id:"wsresource"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wsresource"}},[e._v("#")]),e._v(" WsResource")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('class WsResource(JsonResource):\n    \n        def __init__(self, root):\n            JsonResource.__init__(self)\n            self.root = root\n    \n        def render(self, txrequest):\n            try:\n                return JsonResource.render(self, txrequest).encode(\'utf-8\')\n            except Exception as e:\n                if self.root.debug:\n                    return traceback.format_exc().encode(\'utf-8\')\n                log.err()\n                r = {"node_name": self.root.nodename, "status": "error", "message": str(e)}\n                return self.render_object(r, txrequest).encode(\'utf-8\')\n\n')])])]),t("p",[e._v("它定义了 render 方法，默认返回的是 json 格式，当返回 json 格式出错时返回报错信息。"),t("code",[e._v("JsonResource中为json格式定义了一些头信息")]),e._v("，并且将 return 的数据转为 json 格式数据。")]),e._v(" "),t("h3",{attrs:{id:"resource-resource"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#resource-resource"}},[e._v("#")]),e._v(" resource.Resource")]),e._v(" "),t("p",[t("code",[e._v("Resource")]),e._v("中有很多方法，它的主要功能是定义一个可访问的 Web 资源，并且提供 HTTP 请求的标准、URL 路由设定标准，如"),t("code",[e._v("putChild")]),e._v("等，下图为"),t("code",[e._v("Resource")]),e._v("类的结构图：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/17/166810f2b6605006?w=767&h=1378&f=png&s=59087",alt:""}})]),e._v(" "),t("h2",{attrs:{id:"小结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[e._v("#")]),e._v(" 小结")]),e._v(" "),t("p",[e._v("本小节通过阅读 Scrapyd 中负责 HTML 视图的"),t("code",[e._v("Root")]),e._v("、"),t("code",[e._v("Home")]),e._v("、"),t("code",[e._v("Jobs")]),e._v("类和阅读"),t("code",[e._v("API 中的几个类")]),e._v("，知道了 Scrapyd 的"),t("code",[e._v("视图及 API 构成")]),e._v("，并通过阅读其"),t("code",[e._v("父类")]),e._v("加深了对 Scrapyd 视图的理解。")])])}),[],!1,null,null,null);s.default=r.exports}}]);