(window.webpackJsonp=window.webpackJsonp||[]).push([[279],{635:function(e,s,a){"use strict";a.r(s);var t=a(42),n=Object(t.a)({},(function(){var e=this,s=e.$createElement,a=e._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"常用功能-api-代码编写"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#常用功能-api-代码编写"}},[e._v("#")]),e._v(" 常用功能 API 代码编写")]),e._v(" "),a("p",[e._v("本小节我们使用 render_GET 方法来将首页已有的功能编写成 API ，并且在首页已有功能的基础上逐步加大难度，将 API 的功能变得更丰富。")]),e._v(" "),a("h2",{attrs:{id:"编写-api-爬虫调用情况-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写-api-爬虫调用情况-get"}},[e._v("#")]),e._v(" 编写 API-爬虫调用情况(GET)")]),e._v(" "),a("p",[e._v("已知"),a("code",[e._v("get_ps")]),e._v("方法可以获取项目名列表和爬虫名列表，且向"),a("code",[e._v("get_invokes")]),e._v("方法传入已结束的爬虫运行记录与爬虫名列表，可以计算出被调用、未被调用与调用次数最多的爬虫及对应次数。")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('class ScheduleList(WsResource):\n    @decorator_auth\n    def render_GET(self, request):\n        """爬虫调用情况\n        如被调用过的爬虫、未被调用过的爬虫、被调用次数最多的爬虫名称及次数\n        """\n        finishes = self.root.launcher.finished\n        projects, spiders = get_ps(self)  # 项目/爬虫列表\n        invoked_spider, un_invoked_spider, most_record = get_invokes(finishes, spiders)  # 被调用过/未被调用的爬虫\n        return {"node_name": self.root.nodename, "status": "ok", "invoked_spider": list(invoked_spider),\n        "un_invoked_spider": list(un_invoked_spider), "most_record": most_record}\n\n')])])]),a("p",[e._v("这样就完成了第一个 API，到配置文件中的"),a("code",[e._v("service")]),e._v("级配置下为其增加路由映射关系：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("schedulelist.json = scrapyd.webservice.ScheduleList\n\n")])])]),a("p",[e._v("然后使用浏览器或 Postman 工具对 API 进行测试，演示中笔者使用浏览器：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676e3503a220fa?w=1708&h=914&f=gif&s=604375",alt:""}})]),e._v(" "),a("p",[e._v("从演示动图中可以看到，API 已经可以正常使用了，并且返回了正确的数据:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "invoked_spider": ["tips", "fabias"], "un_invoked_spider": ["players", "quinns", "flu_event", "flu_teams"], "most_record": ["fabias", 2]}\n\n\n')])])]),a("h2",{attrs:{id:"编写-api-爬虫运行时长统计-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写-api-爬虫运行时长统计-get"}},[e._v("#")]),e._v(" 编写 API-爬虫运行时长统计(GET)")]),e._v(" "),a("p",[e._v("有了 Web 界面重构时编写的功能代码，前面这一些 API 写起来比较轻松")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('class RunTimeStats(WsResource):\n    @decorator_auth\n    def render_GET(self, request):\n        """爬虫运行时间统计\n        如平均运行时长、最短运行时间、最长运行时间\n        """\n        finishes = self.root.launcher.finished\n        average, shortest, longest = list(map(str, run_time_stats(finishes)))\n        return {"node_name": self.root.nodename, "status": "ok", "average": average,\n        "shortest": shortest, "longest": longest}\n\n')])])]),a("p",[e._v("增加路由映射：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("runtimestats.json = scrapyd.webservice.RunTimeStats\n\n")])])]),a("p",[e._v("使用测试：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676e861d147de1?w=1713&h=634&f=gif&s=187900",alt:""}})]),e._v(" "),a("p",[e._v("得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "average": "0:00:08.333333", "shortest": "0:00:07", "longest": "0:00:11"}\n\n')])])]),a("h2",{attrs:{id:"编写-api-项目及爬虫数量统计-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写-api-项目及爬虫数量统计-get"}},[e._v("#")]),e._v(" 编写 API-项目及爬虫数量统计(GET)")]),e._v(" "),a("p",[e._v("依样画葫芦：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('class PsnStats(WsResource):\n    @decorator_auth\n    def render_GET(self, request):\n        """ 项目及爬虫数量统计,如项目总数、爬虫总数 """\n        project_nums, spider_nums = list(map(len, get_ps(self)))\n        node_name = self.root.nodename\n        return {"node_name": node_name, "status": "ok", "project_nums": project_nums, "spider_nums": spider_nums}\n\n')])])]),a("p",[e._v("增加路由映射：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("psnstats.json = scrapyd.webservice.PsnStats\n\n")])])]),a("p",[e._v("使用测试：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676ebfbf6dbe75?w=1711&h=300&f=gif&s=218142",alt:""}})]),e._v(" "),a("p",[e._v("得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "project_nums": 2, "spider_nums": 6}\n\n')])])]),a("h2",{attrs:{id:"编写-api-项目与对应爬虫名及爬虫数量-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写-api-项目与对应爬虫名及爬虫数量-get"}},[e._v("#")]),e._v(" 编写 API-项目与对应爬虫名及爬虫数量(GET)")]),e._v(" "),a("p",[e._v("照猫画虎：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('class ProSpider(WsResource):\n    @decorator_auth\n    def render_GET(self, request):\n        """ 项目与对应爬虫名及爬虫数量,如[{"project": i, "spider": "tip, sms", "number": 2}, {……}] """\n        projects, spiders = get_ps(self)  # 项目/爬虫列表\n        pro_spider = get_psn(projects)  # 项目与对应爬虫\n        return {"node_name": self.root.nodename, "status": "ok", "pro_spider": pro_spider}\n\n')])])]),a("p",[e._v("增加路由映射：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("prospider.json = scrapyd.webservice.ProSpider\n\n")])])]),a("p",[e._v("使用测试：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676ee6043022cd?w=1718&h=593&f=gif&s=166337",alt:""}})]),e._v(" "),a("p",[e._v("得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "pro_spider": [{"project": "arts", "spider": "tips", "number": 1}, {"project": "Fabias", "spider": "fabias,flu_event,flu_teams,players,quinns", "number": 5}]}\n\n\n')])])]),a("h2",{attrs:{id:"编写api-爬虫运行时长排行-从高到低-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写api-爬虫运行时长排行-从高到低-get"}},[e._v("#")]),e._v(" 编写API-爬虫运行时长排行 从高到低(GET)")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('\nclass TimeRank(WsResource):\n    """ 爬虫运行时长排行,根据index参数进行切片 """\n\n    def time_rank(self, index=0):\n        """爬虫运行时间排行 从高到低\n        :param index: 排行榜数量 默认返回全部数据，index存在时则切片后返回\n        :return: 按运行时长排序的排行榜数据 [{"time": time, "spider": spider}, {}]\n        """\n        finished = self.root.launcher.finished\n        # 由于dict的键不能重复，但时间作为键是必定会重复的，所以这里将列表的index位置与时间组成tuple作为dict的键\n        tps = {(microsec_trunc(f.end_time - f.start_time), i): f.spider for i, f in enumerate(finished)}\n        result = [{"time": str(k[0]), "spider": tps[k]} for k in sorted(tps.keys(), reverse=True)]  # 已排序\n        ranks = result if not index else result[:index]\n        return ranks\n\n    @decorator_auth\n    def render_GET(self, request):\n        index = int(valid_index(request=request, arg="index", ins="int", default=10))\n        ranks = self.time_rank(index=index)  # 项目/爬虫列表\n        return {"node_name": self.root.nodename, "status": "ok", "ranks": ranks}\n\n')])])]),a("p",[a("strong",[e._v("补充说明")]),e._v("：虽然之前的功能代码已有，但是这个 API 却完成了与之前不同的功能，爬虫运行时长排行的结果数量可以根据参数指定。设定默认的"),a("code",[e._v("index为 0")]),e._v("，"),a("code",[e._v("result")]),e._v("部分将所得结果通过"),a("code",[e._v("sorted 函数")]),e._v("进行排序，最后通过判断是否传入 index 来决定排行的长度。")]),e._v(" "),a("p",[e._v("增加路由映射：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("timerank.json = scrapyd.webservice.TimeRank\n\n")])])]),a("p",[e._v("使用测试：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/16676f7ace722e5d?w=1705&h=517&f=gif&s=423780",alt:""}})]),e._v(" "),a("p",[e._v("当不传入 index 时得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "ranks": [{"time": "0:00:11", "spider": "tips"}, {"time": "0:00:07", "spider": "fabias"}, {"time": "0:00:07", "spider": "fabias"}]}\n\n')])])]),a("p",[e._v("当指定 index=2 时得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "ranks": [{"time": "0:00:11", "spider": "tips"}, {"time": "0:00:07", "spider": "fabias"}]}\n\n')])])]),a("p",[e._v("对比发现，默认排行长度此时为 3，但指定 index 后返回的结果中排行长度为 2，证明排行结果长度控制的测试也通过了。")]),e._v(" "),a("blockquote",[a("p",[e._v("你的任务：控制排行的升降序")])]),e._v(" "),a("p",[e._v("类似 index 传递参数，可以指定升降序 reverse 为 True 或 False，从而实现升降序的控制，这个任务就交给你了！")]),e._v(" "),a("h2",{attrs:{id:"编写-api-爬虫调用次数排行-从高到低-get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#编写-api-爬虫调用次数排行-从高到低-get"}},[e._v("#")]),e._v(" 编写 API-爬虫调用次数排行 从高到低(GET)")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('class InvokeRank(WsResource):\n    """ 爬虫被调用次数排行 根据index参数进行切片 """\n\n    def _invoke_rank(self, finishes, index=10):\n        """获取爬虫被调用次数排行\n        :param: finishes 已运行完毕的爬虫列表\n        :param: spiders 爬虫列表\n        :return: ranks-降序排序的爬虫调用次数列表 [("tip", 3), ()]\n        """\n        invoked_record = Counter(i.spider for i in finishes)\n        ranks = invoked_record.most_common(index) if invoked_record else []\n        return ranks\n\n    @decorator_auth\n    def render_GET(self, request):\n        index = int(valid_index(request=request, arg="index", ins="int", default=10))\n        ranks = self._invoke_rank(self.root.launcher.finished, index=index)  # 项目/爬虫列表\n        return {"node_name": self.root.nodename, "status": "ok", "ranks": ranks}\n\n')])])]),a("p",[a("strong",[e._v("补充说明")]),e._v("：上一个 API 可以通过指定 index 的值来限制排行长度，而这个 API 在此基础上新增了排行的默认长度。将 index 的值设置为 0，这样的话假如排行中数量很多，在不指定 index 的情况下它将会使用默认值进行长度取值，有效控制了排行榜的长度。")]),e._v(" "),a("p",[e._v("增加路由映射：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("invokerank.json = scrapyd.webservice.InvokeRank\n\n")])])]),a("p",[e._v("使用测试：")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/10/15/1667704ae6114549?w=1713&h=443&f=gif&s=268689",alt:""}})]),e._v(" "),a("p",[e._v("当不传入 index 时得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "ranks": [["fabias", 3], ["tips", 2]]}\n\n')])])]),a("p",[e._v("当指定 index=1 时得到返回数据为：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('{"node_name": "gannicus-PC", "status": "ok", "ranks": [["fabias", 3]]}\n\n')])])]),a("p",[e._v("对比发现，默认排行长度此时为 3(小于默认值 10，不被截断)，但指定 index 后返回的结果中排行长度为 1，证明排行结果长度控制的测试也通过了。")])])}),[],!1,null,null,null);s.default=n.exports}}]);