{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[289],{647:function(e,s,t){\"use strict\";t.r(s);var a=t(42),r=Object(a.a)({},(function(){var e=this,s=e.$createElement,t=e._self._c||s;return t(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":e.$parent.slotKey}},[t(\"h1\",{attrs:{id:\"官方源码剖析与-api-详解\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#官方源码剖析与-api-详解\"}},[e._v(\"#\")]),e._v(\" 官方源码剖析与 API 详解\")]),e._v(\" \"),t(\"p\",[e._v(\"本小节我们将对 Scrapyd 原有的 API 做个详细的剖析，只有在了解它原有设计思想与代码风格后，我们才能够照猫画虎，设计出风格相似的代码模块，为之后我们自定义 API 和编写权限验证功能打下基础。\")]),e._v(\" \"),t(\"blockquote\",[t(\"p\",[e._v(\"API 相关代码在 \"),t(\"code\",[e._v(\"webservice.py\")]),e._v(\" 文件中。\")])]),e._v(\" \"),t(\"p\",[e._v(\"根据前面的小节，我们知道 Scrapyd 的视图分为 HTML 和 JSON 两种。我们所看到的数据呈现都是由视图类处理的，比如:\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"* 当我们访问根目录的时候，对应的的是 Home 这个类；\\n* 而访问 Jobs 的时候，对应的的是 Jobs 这个类；\\n* 而爬虫的 API 则是，如启动爬虫的 Schedule 类和查看爬虫列表的 ListSpiders 类。\\n\\n\")])])]),t(\"h2\",{attrs:{id:\"html-视图\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#html-视图\"}},[e._v(\"#\")]),e._v(\" HTML 视图\")]),e._v(\" \"),t(\"p\",[e._v(\"HTML 视图类都写在 Scrapyd 目录下的\"),t(\"code\",[e._v(\"website.py\")]),e._v(\"文件中，里面有三个类：\"),t(\"code\",[e._v(\"Root\")]),e._v(\"、\"),t(\"code\",[e._v(\"Home\")]),e._v(\"、\"),t(\"code\",[e._v(\"Jobs\")]),e._v(\"。\")]),e._v(\" \"),t(\"h3\",{attrs:{id:\"root-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#root-类\"}},[e._v(\"#\")]),e._v(\" Root 类\")]),e._v(\" \"),t(\"p\",[e._v(\"Root 完成了\"),t(\"code\",[e._v(\"Web\")]),e._v(\"路由设置，Scrapyd 一些基础配置的读取设置，日志目录和\"),t(\"code\",[e._v(\"Item\")]),e._v(\"目录与路由配置、项目信息更新等任务，是 Scrapyd 的重要组成部分，其代码结构如下图所示：\")]),e._v(\" \"),t(\"p\",[t(\"img\",{attrs:{src:\"https://user-gold-cdn.xitu.io/2018/10/17/16680fc95682b5f5?w=726&h=735&f=png&s=28547\",alt:\"\"}})]),e._v(\" \"),t(\"h4\",{attrs:{id:\"web-路由设置\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#web-路由设置\"}},[e._v(\"#\")]),e._v(\" Web 路由设置\")]),e._v(\" \"),t(\"p\",[e._v(\"在 Root 类的\"),t(\"code\",[e._v(\"__init__\")]),e._v(\"方法中 Home 类以及 Jobs 类的路由是通过 Twisted 的 putChild 进行配置的：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"self.putChild(b'jobs', Jobs(self, local_items))\\nself.putChild(b'', Home(self, local_items))\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"日志与-item-目录配置\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#日志与-item-目录配置\"}},[e._v(\"#\")]),e._v(\" 日志与 Item 目录配置\")]),e._v(\" \"),t(\"p\",[e._v(\"同样在\"),t(\"code\",[e._v(\"__init__\")]),e._v(\"方法中，先读取日志与 Item 的目录：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"logsdir = config.get('logs_dir')\\nitemsdir = config.get('items_dir')\\n\\n\")])])]),t(\"p\",[e._v(\"然后为它们设置路由：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"self.putChild(b'items', static.File(itemsdir, 'text/plain'))\\nself.putChild(b'logs', static.File(logsdir.encode('ascii', 'ignore'), 'text/plain'))\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"项目信息更新\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#项目信息更新\"}},[e._v(\"#\")]),e._v(\" 项目信息更新\")]),e._v(\" \"),t(\"p\",[e._v(\"当项目变动，比如增删 projects，就会通过 \"),t(\"code\",[e._v(\"update_projects()\")]),e._v(\" 方法进行更新：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def update_projects(self):\\n    self.poller.update_projects()\\n    self.scheduler.update_projects()\\n@property\\ndef poller(self):\\n    return self.app.getComponent(IPoller)\\n    \\n@property\\ndef scheduler(self):\\n    return self.app.getComponent(ISpiderScheduler)\\n\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"scrapyd-的其他一些基础配置等\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#scrapyd-的其他一些基础配置等\"}},[e._v(\"#\")]),e._v(\" Scrapyd 的其他一些基础配置等\")]),e._v(\" \"),t(\"p\",[e._v(\"比如读取配置文件并且将其赋值给变量：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"self.debug = config.getboolean('debug', False)\\nself.runner = config.get('runner')\\nservices = config.items('services', ())\\n\\n\")])])]),t(\"h3\",{attrs:{id:\"home-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#home-类\"}},[e._v(\"#\")]),e._v(\" Home 类\")]),e._v(\" \"),t(\"p\",[e._v(\"Home 负责 Scrapyd 首页的呈现，当我们访问\"),t(\"code\",[e._v(\"http://localhost:6800\")]),e._v(\" 时看到的界面，就是访问 Home 类，它完成了页面 HTML 布局以及当前已有项目\"),t(\"code\",[e._v(\"projects\")]),e._v(\"的名称列表展示。\")]),e._v(\" \"),t(\"p\",[t(\"img\",{attrs:{src:\"https://user-gold-cdn.xitu.io/2018/10/17/16681024695fa0a3?w=750&h=332&f=png&s=15483\",alt:\"\"}})]),e._v(\" \"),t(\"h4\",{attrs:{id:\"init-方法\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#init-方法\"}},[e._v(\"#\")]),e._v(\" \"),t(\"code\",[e._v(\"__init__\")]),e._v(\" 方法\")]),e._v(\" \"),t(\"p\",[e._v(\"Home 类继承自\"),t(\"code\",[e._v(\"resource.Resource\")]),e._v(\"，并且重写了\"),t(\"code\",[e._v(\"__init__\")]),e._v(\"方法，以定义一个 Web 可访问资源：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def __init__(self, root, local_items):\\n    resource.Resource.__init__(self)\\n    self.root = root\\n    self.local_items = local_items\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"对于页面呈现的建议\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#对于页面呈现的建议\"}},[e._v(\"#\")]),e._v(\" 对于页面呈现的建议\")]),e._v(\" \"),t(\"p\",[e._v(\"Rource 中 render 对于页面呈现的建议：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('\"\"\"\\nI delegate to methods of self with the form \\'render_METHOD\\'\\nwhere METHOD is the HTTP that was used to make the\\nrequest. Examples: render_GET, render_HEAD, render_POST, and\\nso on. Generally you should implement those methods instead of\\noverriding this one.\\n\"\"\"\\n\\n')])])]),t(\"h4\",{attrs:{id:\"html-布局的实现以及已有项目列表展示\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#html-布局的实现以及已有项目列表展示\"}},[e._v(\"#\")]),e._v(\" HTML 布局的实现以及已有项目列表展示\")]),e._v(\" \"),t(\"p\",[e._v(\"使用 render 建议的方式，用\"),t(\"code\",[e._v(\"render_GET\")]),e._v(\"来完成页面的呈现：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def render_GET(self, txrequest):\\n   vars = {'projects': ', '.join(self.root.scheduler.list_projects())}\\n   s = \\\"…… ……\\\"\\n   return s.encode('utf-8')\\n\\n\")])])]),t(\"h3\",{attrs:{id:\"jobs-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#jobs-类\"}},[e._v(\"#\")]),e._v(\" Jobs 类\")]),e._v(\" \"),t(\"p\",[e._v(\"Jobs 负责 Scrapyd 首页爬虫运行状态展示、日志记录以及取消爬虫运行，当我们访问\"),t(\"code\",[e._v(\"http://localhost:6800/jobs/\")]),e._v(\"时看到的界面，就是 Jobs 类。同样，它也是继承了 \"),t(\"code\",[e._v(\"resource.Resource\")]),e._v(\"。\")]),e._v(\" \"),t(\"p\",[t(\"img\",{attrs:{src:\"https://user-gold-cdn.xitu.io/2018/10/17/16681085d3992657?w=739&h=927&f=png&s=38600\",alt:\"\"}})]),e._v(\" \"),t(\"h4\",{attrs:{id:\"cancel-与-header\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#cancel-与-header\"}},[e._v(\"#\")]),e._v(\" Cancel 与 Header\")]),e._v(\" \"),t(\"p\",[e._v(\"它设定取消按钮以及爬虫运行信息的列名：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"cancel_button = \\\"\\\"\\\"\\n<form method=\\\"post\\\" action=\\\"/cancel.json\\\">\\n   ……\\n      ……\\n\\\"\\\"\\\".format\\n\\nheader_cols = [\\n    'Project', 'Spider', 'Job', 'PID', 'Start', \\n    'Runtime', 'Finish', 'Log', 'Items', 'Cancel',\\n]\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"设定-css-样式\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#设定-css-样式\"}},[e._v(\"#\")]),e._v(\" 设定 CSS 样式\")]),e._v(\" \"),t(\"p\",[e._v(\"为爬虫日志表格设定 CSS 样式：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def gen_css(self):\\n    css = [\\n        '#jobs>thead td {text-align: center; font-weight: bold}',\\n        '#jobs>tbody>tr:first-child {background-color: #eee}']\\n    ……\\n    return '\\\\n'.join(css)\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"生成表头\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#生成表头\"}},[e._v(\"#\")]),e._v(\" 生成表头\")]),e._v(\" \"),t(\"p\",[e._v(\"生成爬虫日志表格的表头数据：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def prep_row(self, cells):\\n    if not isinstance(cells, dict):\\n        assert len(cells) == len(self.header_cols)\\n    else:\\n        cells = [cells.get(k) for k in self.header_cols]\\n    cells = ['<td>%s</td>' % ('' if c is None else c) for c in cells]\\n    return '<tr>%s</tr>' % ''.join(cells)\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"pending-状态爬虫列表\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#pending-状态爬虫列表\"}},[e._v(\"#\")]),e._v(\" Pending 状态爬虫列表\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def prep_tab_pending(self):\\n    return '\\\\n'.join(\\n        ……\\n    )\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"正在运行的爬虫列表\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#正在运行的爬虫列表\"}},[e._v(\"#\")]),e._v(\" 正在运行的爬虫列表\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def prep_tab_running(self):\\n    return '\\\\n'.join(\\n        ……\\n        for p in self.root.launcher.processes.values()\\n    )\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"运行完毕的爬虫列表及日志记录\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#运行完毕的爬虫列表及日志记录\"}},[e._v(\"#\")]),e._v(\" 运行完毕的爬虫列表及日志记录\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def prep_tab_finished(self):\\n    return '\\\\n'.join(\\n        ……\\n        for p in self.root.launcher.finished\\n    )\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"生成爬虫运行信息表\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#生成爬虫运行信息表\"}},[e._v(\"#\")]),e._v(\" 生成爬虫运行信息表\")]),e._v(\" \"),t(\"p\",[e._v(\"调用以上功能，生成爬虫运行信息表格：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def prep_table(self):\\n    return (\\n            '<table id=\\\"jobs\\\" border=\\\"1\\\">'\\n            self.prep_row(self.header_cols)\\n            self.prep_tab_pending()\\n            self.prep_tab_running()\\n            self.prep_tab_finished()\\n            '…… ……'\\n            )\\n\\n\")])])]),t(\"h4\",{attrs:{id:\"页面渲染\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#页面渲染\"}},[e._v(\"#\")]),e._v(\" 页面渲染\")]),e._v(\" \"),t(\"p\",[e._v(\"使用 render 方法，将生成的爬虫运行信息呈现到\"),t(\"code\",[e._v(\"localhost:6800/jobs/\")]),e._v(\"页面：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"def render(self, txrequest):\\n    doc = self.prep_doc()\\n    txrequest.setHeader('Content-Type', 'text/html; charset=utf-8')\\n    txrequest.setHeader('Content-Length', len(doc))\\n    return doc.encode('utf-8')\\n\\n\")])])]),t(\"h2\",{attrs:{id:\"json-视图\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#json-视图\"}},[e._v(\"#\")]),e._v(\" JSON 视图\")]),e._v(\" \"),t(\"p\",[e._v(\"JSON 视图类都写在 Scrapyd 目录下的\"),t(\"code\",[e._v(\"webservice.py\")]),e._v(\"文件中，里面除了官方文档中提到的 API 外，还有它们父类\"),t(\"code\",[e._v(\"WsResource\")]),e._v(\"。 这里我挑选 3个 API 对应的类进行讲解。\")]),e._v(\" \"),t(\"h3\",{attrs:{id:\"listprojects-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#listprojects-类\"}},[e._v(\"#\")]),e._v(\" ListProjects 类\")]),e._v(\" \"),t(\"p\",[e._v(\"ListProjects 类的作用是返回当前 Scrapyd 服务器上的爬虫项目列表。\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('class ListProjects(WsResource):\\n\\n    def render_GET(self, txrequest):\\n        projects = list(self.root.scheduler.list_projects())\\n        return {\"node_name\": self.root.nodename, \"status\": \"ok\", \"projects\": projects}\\n\\n\\n')])])]),t(\"p\",[e._v(\"它使用 \"),t(\"code\",[e._v(\"render_GET\")]),e._v(\" 方法，所以在浏览器可以输入\"),t(\"code\",[e._v(\"http://localhost:6800/listprojects.json\")]),e._v(\"来访问并获得 JSON 格式的结果，比如：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('{\"status\": \"ok\", \"projects\": [\"AlibabaProject\", \"JDProject\"]}\\n\\n')])])]),t(\"h3\",{attrs:{id:\"listspiders-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#listspiders-类\"}},[e._v(\"#\")]),e._v(\" ListSpiders 类\")]),e._v(\" \"),t(\"p\",[e._v(\"ListSpiders 的作用是返回当前 Scrapyd 服务器上指定项目名的爬虫列表。\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('class ListSpiders(WsResource):\\n\\n    def render_GET(self, txrequest):\\n        args = native_stringify_dict(copy(txrequest.args), keys_only=False)\\n        project = args[\\'project\\'][0]\\n        version = args.get(\\'_version\\', [\\'\\'])[0]\\n        spiders = get_spider_list(project, runner=self.root.runner, version=version)\\n        return {\"node_name\": self.root.nodename, \"status\": \"ok\", \"spiders\": spiders}\\n\\n')])])]),t(\"p\",[e._v(\"它也是使用\"),t(\"code\",[e._v(\"render_GET\")]),e._v(\"方法，但是它需要携带项目名称作为请求参数。如：\"),t(\"code\",[e._v(\"http://localhost:6800/listspiders.json?project=AlibabaProject\")]),e._v(\"返回的结果同样是 json 格式，通过返回的 json 数据，我们知道 AlibabaProject 项目中当前有哪些爬虫。\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('{\"status\": \"ok\", \"spiders\": [\"taobao\", \"1688\", \"tmall\"]}\\n\\n')])])]),t(\"h3\",{attrs:{id:\"deleteproject-类\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#deleteproject-类\"}},[e._v(\"#\")]),e._v(\" DeleteProject 类\")]),e._v(\" \"),t(\"p\",[e._v(\"DeleteProject 的作用是用于删除 Scrapyd 上已有的爬虫项目。\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('class DeleteProject(WsResource):\\n    \\n        def render_POST(self, txrequest):\\n            args = native_stringify_dict(copy(txrequest.args), keys_only=False)\\n            project = args[\\'project\\'][0]\\n            self._delete_version(project)\\n            UtilsCache.invalid_cache(project)\\n            return {\"node_name\": self.root.nodename, \"status\": \"ok\"}\\n    \\n        def _delete_version(self, project, version=None):\\n            self.root.eggstorage.delete(project, version)\\n            self.root.update_projects()\\n\\n')])])]),t(\"p\",[e._v(\"它使用的则是\"),t(\"code\",[e._v(\"render_POST\")]),e._v(\"方法，所以请求的时候我们必须以 post 方式进行，并且它要求携带项目名称作为参数值，如:\"),t(\"code\",[e._v(\"http://localhost:6800/delproject.json -d project=AlibabaProject\")]),e._v(\"在成功删除项目后，更新爬虫项目列表，\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v(\"self.root.update_projects()\\n\\n\")])])]),t(\"p\",[e._v(\"如果操作全部成功，我们得到的响应为：\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('{\"status\": \"ok\"}\\n\\n')])])]),t(\"h2\",{attrs:{id:\"视图父类与-resource\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#视图父类与-resource\"}},[e._v(\"#\")]),e._v(\" 视图父类与 Resource\")]),e._v(\" \"),t(\"p\",[e._v(\"JSON 视图和 HTML 视图的父类是不同的。在 HTML 视图部分，Home 和 Jobs 都继承自\"),t(\"code\",[e._v(\"resource.Resource\")]),e._v(\"，而 JSON 视图部分则继承自\"),t(\"code\",[e._v(\"WsResource\")]),e._v(\"。\")]),e._v(\" \"),t(\"h3\",{attrs:{id:\"wsresource\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#wsresource\"}},[e._v(\"#\")]),e._v(\" WsResource\")]),e._v(\" \"),t(\"div\",{staticClass:\"language- extra-class\"},[t(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[t(\"code\",[e._v('class WsResource(JsonResource):\\n    \\n        def __init__(self, root):\\n            JsonResource.__init__(self)\\n            self.root = root\\n    \\n        def render(self, txrequest):\\n            try:\\n                return JsonResource.render(self, txrequest).encode(\\'utf-8\\')\\n            except Exception as e:\\n                if self.root.debug:\\n                    return traceback.format_exc().encode(\\'utf-8\\')\\n                log.err()\\n                r = {\"node_name\": self.root.nodename, \"status\": \"error\", \"message\": str(e)}\\n                return self.render_object(r, txrequest).encode(\\'utf-8\\')\\n\\n')])])]),t(\"p\",[e._v(\"它定义了 render 方法，默认返回的是 json 格式，当返回 json 格式出错时返回报错信息。\"),t(\"code\",[e._v(\"JsonResource中为json格式定义了一些头信息\")]),e._v(\"，并且将 return 的数据转为 json 格式数据。\")]),e._v(\" \"),t(\"h3\",{attrs:{id:\"resource-resource\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#resource-resource\"}},[e._v(\"#\")]),e._v(\" resource.Resource\")]),e._v(\" \"),t(\"p\",[t(\"code\",[e._v(\"Resource\")]),e._v(\"中有很多方法，它的主要功能是定义一个可访问的 Web 资源，并且提供 HTTP 请求的标准、URL 路由设定标准，如\"),t(\"code\",[e._v(\"putChild\")]),e._v(\"等，下图为\"),t(\"code\",[e._v(\"Resource\")]),e._v(\"类的结构图：\")]),e._v(\" \"),t(\"p\",[t(\"img\",{attrs:{src:\"https://user-gold-cdn.xitu.io/2018/10/17/166810f2b6605006?w=767&h=1378&f=png&s=59087\",alt:\"\"}})]),e._v(\" \"),t(\"h2\",{attrs:{id:\"小结\"}},[t(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#小结\"}},[e._v(\"#\")]),e._v(\" 小结\")]),e._v(\" \"),t(\"p\",[e._v(\"本小节通过阅读 Scrapyd 中负责 HTML 视图的\"),t(\"code\",[e._v(\"Root\")]),e._v(\"、\"),t(\"code\",[e._v(\"Home\")]),e._v(\"、\"),t(\"code\",[e._v(\"Jobs\")]),e._v(\"类和阅读\"),t(\"code\",[e._v(\"API 中的几个类\")]),e._v(\"，知道了 Scrapyd 的\"),t(\"code\",[e._v(\"视图及 API 构成\")]),e._v(\"，并通过阅读其\"),t(\"code\",[e._v(\"父类\")]),e._v(\"加深了对 Scrapyd 视图的理解。\")])])}),[],!1,null,null,null);s.default=r.exports}}]);","extractedComments":[]}